---
title: "TwitterAPI"
output: html_document
---
I decided to use the rTweet package to scrape news organizations thatreport on the subject of cryptocurrencies

```{r,eval=FALSE}
library(rtweet)


## whatever name you assigned to your created app
appname <- "rtweet_ce"

## api key
key <- "gaZ7oOlxa5M7e0ndqQRX4ZCXy"

## api secret 
secret <- "0gBA23qT2ZgW2rUHRCwcObH8ScjeC9uAugH2eQS7r8o6EodD6r"

## create token named "twitter_token"
twitter_token <- create_token(
  app = appname,
  consumer_key = key,
  consumer_secret = secret)


```


Tweets from Coindesk.com

```{r,eval=FALSE}
coindesk <- get_timelines(c("coindesk"), n = 3200)
```

Tweets from Cointelegraph.com

```{r,eval=FALSE}
cointelegraph <- get_timelines(c("Cointelegraph"), n = 3200, include_rts=FALSE)
```


Tweets from The Merkle
```{r,eval=FALSE}
TheMerkle <- get_timelines(c("themerklenews"), n = 3200,include_rts=FALSE)
```

Tweets from CCN

```{r,eval=FALSE}
CCN <- get_timelines(c("CryptoCoinsNews"), n = 3200,include_rts=FALSE)
```

Tweets from Bitcoin News

```{r,eval=FALSE}
BitcoinNews <- get_timelines(c("BTCTN"), n = 3200,include_rts=FALSE)
```


```{r}
tm_cleaner <- function(corpus, stop=stopwords("en"), rm_num=TRUE) {
  # Cleans a corpus object of spaces,numbers,case,stop words, and punctuation
  # Args:
  # Corpus: Object to be cleaned
  # stop=stopwords(""): define dictionary from which corrsponding stopwords will be pulled
  # rm_num: True or False argument
  #
  #Returns: 
  #Cleaned corpus object where number of rows is equal to input cprpus object.
  #
  require(tm)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stop)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\w+", "", x)))
  return(corpus)
}
```



```{r}
sentimentoutputconttweet <- function(data)  {
#Generates data frame of tweet and continous sentiment analysis results (positive/negative) from the following dictionaries: General Inquirer(Harvard-IV), Loughran-McDonald financial dictionary, and QDAP (polarity words from qdap package).   
#  
#Adds a variable called "score" where the results of the Sentiment dictionary vaules are added together to form an aggregate score. 
#  
#Returns unique rows, eliminating identical tweets  
#  
#Creates variable called "var", a calculation of the varience of the sentiment analysis ratings.   
#  
#Arg:
#data- dataframe genreated from scrapping with rtimes package 

#Return
#analysis- dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date),Variance   
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)
library(textclean)

test <- replace_non_ascii(data$text, remove.nonconverted = TRUE)  
  
tweets <- test

tweetcorpus <- Corpus(VectorSource(tweets))

tweetclean <- tm_cleaner(tweetcorpus)

sentimentnum <- analyzeSentiment(tweetclean)

GI <-(sentimentnum$SentimentGI)
LM <-(sentimentnum$SentimentLM)
QDAP <-(sentimentnum$SentimentQDAP)

tweetoriginal <- factor(data$text)

Score <- (GI + LM + QDAP)

var <- 0.5* ((((GI)-(Score/3))^2) + (((LM)-(Score/3))^2) + (((QDAP)-(Score/3))^2))

analysis <- data.frame(Score=as.numeric(GI + LM + QDAP),
  GI=as.numeric(GI),
  LM=as.numeric(LM),
  QDAP=as.numeric(QDAP),
  Headline=factor(data$text),
  date=as.Date(data$created_at),
  Var=as.numeric(var),
  stringsAsFactors=FALSE)

analysis <- analysis[complete.cases(analysis), ]

analysis2 <- unique(analysis)

return(analysis2) 

}
```


```{r}
firsttweetplot <-function (data,title){
#Function to create visually appealing plot of sentiment scores of each tweet after passing dataset generated from rtweet.   
#  
#Args:  
#Data - Dataset generated from rtweet package (dataframe)  
#title- Title of Plot (character)   
library(plotly)
library(ggplot2)
ggplot(data) + geom_point(aes(x=date,y=Score, colour = Score <0)) +
  scale_colour_manual(name = 'Score < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date') + ggtitle(title, subtitle = ("Score"))  
  
}
```

Plots of the datasets after sentiment analysis

```{r}
coindeskdata <- sentimentoutputconttweet(coindesk)
firsttweetplot(coindeskdata,"Coindesk")
```


```{r}
BitcoinNewsData <- sentimentoutputconttweet(BitcoinNews)
firsttweetplot(BitcoinNewsData,"BitcoinNews")
```


```{r}
cointelegraphdata <- sentimentoutputconttweet(cointelegraph)
firsttweetplot(cointelegraphdata,"Cointelegraph")
```


```{r}
BitcoinMagazinedata <- sentimentoutputconttweet(BitcoinMagazine)
firsttweetplot(BitcoinMagazinedata,"BitcoinMagazine")
```


```{r}
TheMerkledata <- sentimentoutputconttweet(TheMerkle)
firsttweetplot(TheMerkledata,"TheMerkle")
```


```{r}
CCNdata <- sentimentoutputconttweet(CCN)
firsttweetplot(CCNdata,"CCN")
```


Merging all datasets into one

```{r}
alltweets   <-   rbind(CCNdata,TheMerkledata)
alltweets   <-   rbind(alltweets,BitcoinNewsData)
alltweets   <-   rbind(alltweets,cointelegraphdata)
alltweets   <-   rbind(alltweets,coindeskdata)
firsttweetplot(alltweets,"alltweets")
```


```{r}
aggregatescoretweet <- function(data) {
#Function to aggregate the sentiment scores of all tweets from a particular date into one aggregate score per date
  
#Args:  
#data -  dataframe with Score,GI,LM,QDAP,Headline,Date (Dataframe)
#Returns:  
#sums - dataframe with Date and Corresponding Aggregate Score (Dataframe)  
    
library(stringr)
library(dplyr)

sums <- data %>%
           group_by(date) %>%
           summarise(Score = sum(Score))

return(sums)  
  
}

```


```{r}
test <- aggregatescoretweet(alltweets)
```

```{r}
firsttweetplot(test,test)
```

I recognize that this data might not be ideal since the number of tweets in a day can run out the aggregate score. So I make a function that rectifies this by averaging the aggregate score by the amount of tweets that contribute to that score.

```{r}
aggregatescoreproper <- function(data) {
#Function to aggregate the sentiment scores of all tweets from a particular date into one aggregate score per date and average the aggregate score by the amount of tweets that contribute to that score.
#Args:  
#data -  dataframe with Score,GI,LM,QDAP,Headline,Date (Dataframe)
#Returns:  
#sums - dataframe with Date and Corresponding Average Aggregate Score (Dataframe)   
#
#    
library(stringr)
library(dplyr)


table <- data %>%
  group_by(date) %>%
  summarise (n = n())


sums <- data %>%
           group_by(date) %>%
           summarise(Score = sum(Score)) 

result <- data.frame(date=as.Date(table$date),
                     Score=as.numeric((sums$Score)/(table$n)))

return(result)  
  
}

```

```{r}
alltweetsaggscoreavg <- aggregatescoreproper(alltweets)
firsttweetplot(alltweetsaggscoreavg,"Average Score Per Day") 
```
-------------------------------------------------------------------------------------------

Cluster Analysis

I decided to move to compare the sentiment analysis scores that I have generated with cryptocurrency market closing vaules. I first work to merge the market data with the sentiment analysis data that I had collected.  

First I will scrape cryptocurrency data using the Crypto package.
Documentation about this package is avalible at https://cran.r-project.org/web/packages/crypto/index.html

I scrape only the data from the date range that I have tweets for, 2017-09-07 to 2018-04-30. 

```{r,eval=FALSE}
library(crypto)

will_i_get_rich <- getCoins(start_date = '20170907', end_date = '20180430')
```

Subset the dataframe generated from the crypto package that has top 15 cryptocurrencies into one dataframe containg a date column and 15 more columns that are named after their respective cryptocurrencies.

```{r}

Bitcoin <- subset(will_i_get_rich,will_i_get_rich$ranknow == 1 )
myvars <- c("date", "close")
Bitcoin <- Bitcoin[myvars]
Bitcoin$Bitcoin <- Bitcoin$close
Bitcoin$close <- NULL

Ethereum <- subset(will_i_get_rich,will_i_get_rich$ranknow == 2 )
Ethereum <- Ethereum[myvars]
Ethereum$Ethereum <- Ethereum$close
Ethereum$close <- NULL

Ripple <- subset(will_i_get_rich,will_i_get_rich$ranknow == 3 )
Ripple <- Ripple[myvars]
Ripple$Ripple <- Ripple$close
Ripple$close <- NULL

Bitcoin_Cash <- subset(will_i_get_rich,will_i_get_rich$ranknow == 4 )
Bitcoin_Cash <- Bitcoin_Cash[myvars]
Bitcoin_Cash$Bitcoin_Cash <- Bitcoin_Cash$close
Bitcoin_Cash$close <- NULL

EOS <- subset(will_i_get_rich,will_i_get_rich$ranknow == 5 )
EOS <- EOS[myvars]
EOS$EOS <- EOS$close
EOS$close <- NULL

Cardano <- subset(will_i_get_rich,will_i_get_rich$ranknow == 6 )
Cardano <- Cardano[myvars]
Cardano$Cardano <- Cardano$close
Cardano$close <- NULL

Stellar <- subset(will_i_get_rich,will_i_get_rich$ranknow == 7 )
Stellar <- Stellar[myvars]
Stellar$Stellar <- Stellar$close
Stellar$close <- NULL

Litecoin <- subset(will_i_get_rich,will_i_get_rich$ranknow == 8 )
Litecoin <- Litecoin[myvars]
Litecoin$Litecoin <- Litecoin$close
Litecoin$close <- NULL

TRON <- subset(will_i_get_rich,will_i_get_rich$ranknow == 9 )
TRON <- TRON[myvars]
TRON$TRON <- TRON$close
TRON$close <- NULL

NEO <- subset(will_i_get_rich,will_i_get_rich$ranknow == 10 )
NEO<- NEO[myvars]
NEO$NEO <- NEO$close
NEO$close <- NULL

IOTA <- subset(will_i_get_rich,will_i_get_rich$ranknow == 11 )
IOTA <- IOTA[myvars]
IOTA$IOTA <- IOTA$close
IOTA$close <- NULL

Monero <- subset(will_i_get_rich,will_i_get_rich$ranknow == 12 )
Monero <- Monero[myvars]
Monero$Monero <- Monero$close
Monero$close <- NULL

Dash <- subset(will_i_get_rich,will_i_get_rich$ranknow == 13 )
Dash <- Dash[myvars]
Dash$Dash <- Dash$close
Dash$close <- NULL

NEM <- subset(will_i_get_rich,will_i_get_rich$ranknow == 14 )
NEM <- NEM[myvars]
NEM$NEM <- NEM$close
NEM$close <- NULL

Tether <- subset(will_i_get_rich,will_i_get_rich$ranknow == 15 )
Tether <- Tether[myvars]
Tether$Tether <- Tether$close
Tether$close <- NULL

Data <- Reduce(function(x, y) merge(x, y, all=TRUE), list(tweetscoresubset, Bitcoin, Ethereum, Ripple,Bitcoin_Cash,EOS,Cardano,Stellar,Litecoin,TRON,NEO,IOTA,Monero,Dash,NEM,Tether))

str(Data)
```

Create a dataset without date for correlation plot.

```{r}
datatest <- Data

datatest$date <- NULL
```

Correlation plot. Remove the Cardano and TRON Cryptocurrencies as they disrupt the plot, they contain many NA vaules as they were created after the beginning of the date range I am working with.

```{r}
library(corrplot)
datacorr <- datatest
datacorr$Cardano <- NULL
datacorr$TRON <- NULL
numeric_cor <- cor(datacorr)
corrplot(numeric_cor)

```


```{r}
View(numeric_cor)
```


When repeating the previous analysis, but filtering varience to only include tweets that had a calculated varience below 0.01.  

```{r}
tweetsvar <- subset(alltweets, Var < 0.01)
tweetsvar <- subset(tweetsvar, date < "2018-05-01")

tweetsvar <- aggregatescoreproper(tweetsvar)

Datavartest <- Reduce(function(x, y) merge(x, y, all=TRUE), list(tweetsvar, Bitcoin, Ethereum, Ripple,Bitcoin_Cash,EOS,Cardano,Stellar,Litecoin,TRON,NEO,IOTA,Monero,Dash,NEM,Tether))
```


```{r}
datavar <- Datavartest

datavar$date <- NULL

library(corrplot)
datacorrvar <- datavar
datacorrvar$Cardano <- NULL
datacorrvar$TRON <- NULL
numeric_var <- cor(datacorrvar)
corrplot(numeric_var)

```

```{r}
c <- numeric_cor[1,]
v <- numeric_var[1,]
```

```{r}
c
```


```{r}
v
```








