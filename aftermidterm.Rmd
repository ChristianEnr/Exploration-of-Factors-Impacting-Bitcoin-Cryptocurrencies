---
title: "MIDTERNONWARD"
output: html_document
---

R times Web Scraper

```{r, eval=FALSE}
Sys.setenv(NYTIMES_AS_KEY = "b4969dfbe67548568bf022335e4bd5fb")
  
bit <- as_search(q="bitcoin", begin_date = "20100101", end_date = '20180401', all_results = TRUE)  

crypt <- as_search(q="cryptocurrency", begin_date = "20100101", end_date = '20180401', all_results = TRUE)  
```

bitcoin yields more data points (1154 to cryto's 125)

want to use sentiment analysis to rate headlines

bring in tm_cleaner function

```{r}
tm_cleaner <- function(corpus, stop=stopwords("en"), rm_num=TRUE) {
  require(tm)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stop)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\w+", "", x)))
  return(corpus)
}

```

created function to generate dataframes of sentiment analysis with diffrent dictionarys
where 
"dataNYT" is a dataframe generated from rtimes. 

```{r, eval=FALSE}
sentimentoutput <- function(dataNYT)  {
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   
  
headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-convertToBinaryResponse(sentimentnum$SentimentGI)
LM <-convertToBinaryResponse(sentimentnum$SentimentLM)
QDAP <-convertToBinaryResponse(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

GI_num <- ifelse(GI =="positive",1,-1) 
LM_num <- ifelse(LM =="positive",1,-1) 
QDAP_num <- ifelse(QDAP =="positive",1,-1) 

analysis <- data.frame(Score=factor(GI_num + LM_num + QDAP_num),
  GI=factor(GI),
  LM=factor(LM),
  QDAP=factor(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

return(analysis)

}

```

Have a score in this set, to factor in responses from all dictionaries 


```{r , eval=FALSE}
bitdata <- sentimentoutput(bit)

cryptdata <- sentimentoutput(crypt)

```

Looking at the data, there are a number of headlines that dont seem to have much to do with bitcoin or crypto currency. will delete rows if no string of bitcoin and cryptocurrency exist

```{r , eval=FALSE}

library(stringr)
library(dplyr)
cleanbit <- bitdata %>%
  filter(str_detect(Headline, "Bitcoin"))

cleancrypt <- cryptdata %>%
  filter(str_detect(Headline, "Cryptocurrencies"))
```

will graph this data see what we find

```{r , eval=FALSE}
library(ggplot2)
ggplot(cleanbit) + 
  geom_point(aes(x=date, y=Score)) 

```

There appears to be a high amount of "good" headlines concerning bitcoin (scores of three). This seems to be promising coorelating with the increse in market vaule, but I would like to create some kind of density graph so we can see where media coverage was at its highest.

```{r , eval=FALSE}
library(dplyr)
table <- cleanbit %>%
  group_by(date) %>%
  summarise (n = n())
```

```{r , eval=FALSE}
ggplot(table) + geom_density(aes(date,fill="red")) + ggtitle("NYT Interest in Bitcoin", subtitle = "Smoothed")
```

it seems NYT intrest peaked around early 2014. 

plotting bitcoin again

```{r , eval=FALSE}
alldata$date <- as.Date(alldata$date,"%m/%d/%Y")

ggplot(subset(alldata,slug %in% c("bitcoin"))) + 
  geom_line(aes(x=date, y=close)) 

```

function to create news frequency table

```{r}
freqnews <- function(analyze, word) {
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


return(table)

}
```

```{r, eval=FALSE}
table <- freqnews(bitdata,"Bitcoin")


ggplot(table) + 
  geom_line(aes(x=date, y=n)) 
```


let me create a function that i can drop any rtimes file into to rate and aggregate 

"analyze" is a dataset generated from previous "sentiment output"" function
"word" is a chracter string that if a row doesnt not contain, it is deleted

```{r}
aggregatescore <- function(analyze,word) {
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

return(sums)  
  
}


```

```{r, eval=FALSE}
cleanbitagg <- aggregatescore(bitdata,"Bitcoin")

```

When we plot with a dataset cleaned for the chracter string bitcoin in the headlines we find that most coverage is postive when we add the score for each date. 


```{r, eval=FALSE}
library(ggplot2)
ggplot(cleanbitagg) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date')
```

let me pull the dates of that have a score larger than 5, and see what they have in common

```{r}

topheadline <- function(analyze,word, condition) {
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums, eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

return(complete)

}

```

```{r, eval=FALSE}
topheadline<- topheadline(bitdata,"Bitcoin","sum > 0")
```

When using the above function to isolate headlines form days with a high news score I find reundent headlines. This can affect the score and make my results less meaningful. Will creat function to eleminate duplicates and hopefully near duplicates. 


I will modify my sentimentoutput function 

```{r}
sentimentoutput <- function(dataNYT)  {
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   

headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-convertToBinaryResponse(sentimentnum$SentimentGI)
LM <-convertToBinaryResponse(sentimentnum$SentimentLM)
QDAP <-convertToBinaryResponse(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

GI_num <- ifelse(GI =="positive",1,-1) 
LM_num <- ifelse(LM =="positive",1,-1) 
QDAP_num <- ifelse(QDAP =="positive",1,-1) 

analysis <- data.frame(Score=factor(GI_num + LM_num + QDAP_num),
  GI=factor(GI),
  LM=factor(LM),
  QDAP=factor(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

analysis2 <- unique(analysis)

return(analysis2)

}
```
test 

```{r, eval=FALSE}
bitdata <- sentimentoutput(bit)
```

1154 to 1145, the outliers may have been just duplicates.


Let me summerize work flow at this point

1. genreate dataset from rtimes

2. drop dataset into "sentimentoutput" function to rate each headline. Generates table with score,GI,LM,QDAP rating,headline,date. Score is an aggregate sum where each rating is either 1=postive or -1 negative.  

Sentimentoutput <- function(dataNYT)

dataNYT-dataset genreated from scrapping with rtimes package

--------------------------------------------
From there we can use the following functions: freqnews,aggregatescore, or topheadline.
---------------------------------------------

aggregatescore - genreates table with date and score. Score is sum of headline news scores. 

aggregatescore <- function(analyze,word)

analyze-dataset generated from sentimentoutput

word-chracter string filter (i.e. "bitcoin")

---------------------------------------------

freqnews - generates table with date and number of headlines.

freqnews <- function(analyze, word)

analyze-dataset generated from sentimentoutput

word-chracter string filter (i.e. "bitcoin")

---------------------------------------------

topheadline - filters table to include headlines from dates with that have a score within a specified range

topheadline <- function(analyze,word, x)

analyze-dataset generated from sentimentoutput

word-chracter string filter (i.e. "bitcoin")

condition-specifed range (i.e. "10 > sum" )


--------------------------------------------

wordc-generates wordcloud, from rtimes dataset 
wordc <- function(analyze,word,condition,y)

analyze-dataset generated from sentimentoutput

word-chracter string filter (i.e. "bitcoin")

condition-specifed range (i.e. "10 > sum" )

y - max words (i.e. 10)

-------------------------------------------
Lets create a word cloud of the top headlines based on the topheadline function.


```{r}
wordc <- function(analyze,word,condition,y) {
library(tm)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(dplyr)
library(SnowballC)

clean <- analyze %>%
  filter(str_detect(Headline, word))

clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums,eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

Corpus <- Corpus(VectorSource(complete$Headline))


wordcloud(Corpus, max.words = y, random.order = FALSE)

}

```

```{r}
wordc(bitdata,"Bitcoin","sum > 3",100)
```


```{r}
wordcnofilter <- function(analyze,condition,y) {
library(tm)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(dplyr)
library(SnowballC)

clean <- analyze 

clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums,eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

Corpus <- Corpus(VectorSource(complete$Headline))


wordcloud(Corpus, max.words = y, random.order = FALSE)

}
```

```{r}
wordcnofilter(bitdata,"sum > 0",100)

```

Unfortenetly, our headlines really don't tell much, even when we peform a string detect to try to get rid of headlines that likely have nothing to do with cryptocurrency.



Revisiting the previous sentimentoutput funtion, it will be useful to take a look at an apporch that does not reduce the sentiment analysis to a bionary. 

The convert to bionary function looks at sentiment(dictionary) to make the jugement on if to count a headline as positive or negative. So instead ofrecoding, why not just add these? 

```{r}
sentimentoutput2 <- function(dataNYT)  {
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   

headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-(sentimentnum$SentimentGI)
LM <-(sentimentnum$SentimentLM)
QDAP <-(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

analysis <- data.frame(Score=as.numeric(GI + LM + QDAP),
  GI=as.numeric(GI),
  LM=as.numeric(LM),
  QDAP=as.numeric(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

analysis2 <- unique(analysis)

return(analysis2) 

}
```


```{r}
bitdata2 <- sentimentoutput2(bit) 
```


```{r}
aggnon <- aggregatescore(bitdata2,"Bitcoin")

```


when score is bianary

```{r}
library(ggplot2)
ggplot(cleanbitagg) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date')
```


when it is not

```{r}
library(ggplot2)
ggplot(aggnon) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date')
```



I like the above graph more, as the binary decision allows for less "neutrality". We see from the graph that bad and good news both seem to exist in bursts, the notable ones being:

early 2014

Mid 2014

early 2018



WORDCLOUD for headlines with score above 0
```{r}
wordc(bitdata2,"Bitcoin","sum > 0",50)
```

Wordcloud for headlines with a score < 0
```{r}
wordc(bitdata2,"Bitcoin","sum < 0",50)
```



When looking at both wordclouds, the positve wordcloud makes more sense with many words suggesting something of a positve nature: winner, accepting, fortune, donations, surges.

However, the negative wordcloud, has less obvious giveaways, other than :fraus and loss and falls.

But both wordclouds are not as convicing as they could be, and perhaps frequency of the headlines would be more valuble in the long run.

It may be possible to caulcate variance amoung the dictionary ratings, and drop headlines with too much variance

```{r}


bitdata2$var <- 0.5* ((((bitdata2$GI)-(bitdata2$Score/3))^2) + (((bitdata2$LM)-(bitdata2$Score/3))^2) + (((bitdata2$QDAP)-(bitdata2$Score/3))^2))

```


```{r}
bitdata3filter <- bitdata2 %>%
  filter(str_detect(Headline, "Bitcoin"))

bitdata3 <- filter(bitdata3filter, var < 0.01 )

```

accounting varience, how do word clouds change?

```{r}
wordcnofilter(bitdata3,"sum < 0",100)
```


------------------------------------------------


moving to looking at web scraping 

```{r}


parse <- function(url,Raw=TRUE) {
library(rvest)
library(SentimentAnalysis)
library(rvest)
library(stringr)
library(tm)
    
  scraping <- read_html(url)
  
  p_text <- scraping %>%
    html_nodes("a") %>%
    html_text()
  
  cor <- Corpus(VectorSource(clean))
  
  text <- tm_cleaner(cor)
  
  sentiment <- analyzeSentiment(text)

  sentiment <- convertToBinaryResponse(sentiment$SentimentQDAP)
  
  text <- lapply(text, '[[', 1) 
  text<- unlist(text, recursive = TRUE, use.names = TRUE)
  
  df_true <- data.frame(
    Score=factor(sentiment), 
    Text=factor(clean), 
    stringsAsFactors=FALSE) 
  
  df_false <- data.frame(
    Score=factor(sentiment), 
    Text=factor(text), 
    stringsAsFactors=FALSE) 

if (Raw==TRUE) return(df_true)
if (Raw==FALSE) return(df_false)  
  
}


```

```{r}
result <- parse("https://bitcointalk.org/index.php?board=1.0", TRUE)
```


web scraping for bitcoinforums.org

```{r}

bitcoinforums <- function(url,time,start)
  
{
 #time is 1:5
  #url is "https://bitcointalk.org/index.php?board=1.0"  of first page
 #start is number at end of url
scraping <- read_html(url)
  
  p_text <- scraping %>%
    html_nodes("span") %>%
    html_text()
  
  # clean first few rows and last three that have text from mods that isn't relevant
    
  p_text <- p_text[12:( length(p_text) - 3 )]
  p_text3 <- p_text
  
 for (i in time) {
   
   
   
  page <- (start + (.40)* i )  
  url <- paste0("https://bitcointalk.org/index.php?board=", page)
  
     
     
  scrape <- read_html(url)  
    
  
  
  p_text2 <- scrape %>%
    html_nodes("span") %>%
    html_text()
  
  
 

  p_text3 <- dplyr::union(p_text3,p_text2)  
  
 }

return(p_text3)
  
}
 
```




```{r}
marketdiscuss<- bitcoinforums("https://bitcointalk.org/index.php?board=5.0",1:10,5.0)

```


use this code to clean text files


```{r}

  cor <- Corpus(VectorSource(clean))
  
  text <- tm_cleaner(cor)
  
  sentiment <- analyzeSentiment(text)

  sentiment <- convertToBinaryResponse(sentiment$SentimentQDAP)
  
  text <- lapply(text, '[[', 1) 
  text<- unlist(text, recursive = TRUE, use.names = TRUE)
  
  df_true <- data.frame(
    Score=factor(sentiment), 
    Text=factor(clean), 
    stringsAsFactors=FALSE) 
  

View(p_text)
```




