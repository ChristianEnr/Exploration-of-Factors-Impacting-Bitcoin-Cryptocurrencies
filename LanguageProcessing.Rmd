---
title: "LanguageProcessing"
output: html_document
---

Having looked at the data, I was motivated to look for particular news events that may have defined cryptocurrency affected it's market value and reach some insight into what causes cryptocurrency to fluctuate. The first step of this process was to take advantage of Rtimes package.

Rtimes package is a package that acts as an interface between R and the New York Times API
Documentation available at https://cran.r-project.org/web/packages/rtimes/index.html


```{r, eval=FALSE}
Sys.setenv(NYTIMES_AS_KEY = "b4969dfbe67548568bf022335e4bd5fb")
```


I first used the API to generate all headlines with the keyword "bitcoin" from January of 2010-April of 2018. 

```{r}
bit <- as_search(q="bitcoin", begin_date = "20100101", end_date = '20180401', all_results = TRUE)  
```


I first used the API to generate all headlines with the keyword "cryptocurrency" from January of 2010-April of 2018. 

```{r}
crypt <- as_search(q="cryptocurrency", begin_date = "20100101", end_date = '20180401', all_results = TRUE) 
```


As these datasets are generated, we find that the observations generated from the "bitcoin" query are much larger in number than the "cryptocurrency" query.

```{r}
nrow(bit$data)
nrow(crypt$data)
```


I am motivated at this point to take the headlines that have been retrieved from the package and use language processing tools to plot rating against market vaule and see if there are any observable trends. 


First I will define some functions that will help in the process.



```{r}
tm_cleaner <- function(corpus, stop=stopwords("en"), rm_num=TRUE) {
  # Cleans a corpus object of spaces,numbers,case,stop words, and punctuation
  # Args:
  # Corpus: Object to be cleaned
  # stop=stopwords(""): define dictionary from which corrsponding stopwords will be pulled
  # rm_num: True or False argument
  #
  #Returns: 
  #Cleaned corpus object where number of rows is equal to input cprpus object.
  #
  require(tm)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stop)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\w+", "", x)))
  return(corpus)
}
```

I developed the following function to easily replicate the sentiment analysis process.

```{r, eval=FALSE}
sentimentoutputbi <- function(dataNYT)  {
#Generates data frame of NYT headlines and binary sentiment analysis results (positive/negative) from the following dictionaries: General Inquirer(Harvard-IV), Loughran-McDonald financial dictionary, and QDAP (polarity words from qdap package).   

#Also adds a variable called "score" where the results of the dictionaries are converted to a numeric factor (1="positive" or -1="negative") and added together to form an aggregate score
  
#Arg:
#dataNYT- dataframe genreated from scrapping with rtimes package 

#Return
#analysis- dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date)    
      
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   
  
headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-convertToBinaryResponse(sentimentnum$SentimentGI)
LM <-convertToBinaryResponse(sentimentnum$SentimentLM)
QDAP <-convertToBinaryResponse(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

GI_num <- ifelse(GI =="positive",1,-1) 
LM_num <- ifelse(LM =="positive",1,-1) 
QDAP_num <- ifelse(QDAP =="positive",1,-1) 

analysis <- data.frame(Score=factor(GI_num + LM_num + QDAP_num),
  GI=factor(GI),
  LM=factor(LM),
  QDAP=factor(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

return(analysis)

}

```


Taking the dataframes generated from query to the Rtimes package and run them through the sentiment analysis function that I just defined.

```{r , eval=FALSE}
bitdata <- sentimentoutputbi(bit)

cryptdata <- sentimentoutputbi(crypt)

```


Looking at the data, there are a number of headlines that don't seem to have much to do with bitcoin or crypto currency. I will delete rows if no string containing "bitcoin" or "cryptocurrency" exist.

```{r}
View(bitdata)
View(cryptdata)
```


```{r , eval=FALSE}

library(stringr)
library(dplyr)
cleanbit <- bitdata %>%
  filter(str_detect(Headline, "Bitcoin"))

cleancrypt <- cryptdata %>%
  filter(str_detect(Headline, "Cryptocurrencies"))
```

Graph of Cleanbit (Score of Dataset that concerns headlines that mention bitcoin)

```{r , eval=FALSE}
library(ggplot2)
ggplot(cleanbit) + 
  geom_point(aes(x=date, y=Score)) 
```
Graph of Cleancrypt (Score of Dataset that concerns headlines that mention bitcoin)

```{r , eval=FALSE}
library(ggplot2)
ggplot(cleancrypt) + 
  geom_point(aes(x=date, y=Score))
```

There appears to be a high amount of "good" headlines concerning bitcoin (scores of three). This seems to be promising coorelating with the increse in market vaule, but I would like to create some kind of density graph so we can see where media coverage was at its highest.

```{r , eval=FALSE}
library(dplyr)
table <- cleanbit %>%
  group_by(date) %>%
  summarise (n = n())
```


Looking at the result, it appears that media intrest from the NYT in Bitcoi, peaked in 2014. 

```{r , eval=FALSE}
ggplot(table) + geom_density(aes(date,fill="red")) + ggtitle("NYT Interest in Bitcoin", subtitle = "Smoothed")
```




```{r}
freqnews <- function(analyze, word) {
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


return(table)

}
```





