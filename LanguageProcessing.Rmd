---
title: "LanguageProcessing"
output: html_document
---

Having looked at the data, I was motivated to look for particular news events that may have defined cryptocurrency affected it's market value and reach some insight into what causes cryptocurrency to fluctuate. The first step of this process was to take advantage of Rtimes package.

Rtimes package is a package that acts as an interface between R and the New York Times API
Documentation available at https://cran.r-project.org/web/packages/rtimes/index.html


```{r, eval=FALSE}
library(rtimes)
Sys.setenv(NYTIMES_AS_KEY = "b4969dfbe67548568bf022335e4bd5fb")
```


I first used the API to generate all headlines with the keyword "bitcoin" from January of 2010-April of 2018. 

```{r, eval=FALSE}
bit <- as_search(q="bitcoin", begin_date = "20100101", end_date = '20180401', all_results = TRUE)  
```


I first used the API to generate all headlines with the keyword "cryptocurrency" from January of 2010-April of 2018. 

```{r,eval=FALSE}
crypt <- as_search(q="cryptocurrency", begin_date = "20100101", end_date = '20180401', all_results = TRUE) 
```


As these datasets are generated, we find that the observations generated from the "bitcoin" query are much larger in number than the "cryptocurrency" query.

```{r}
nrow(bit$data)
nrow(crypt$data)
```


I am motivated at this point to take the headlines that have been retrieved from the package and use language processing tools to plot rating against market vaule and see if there are any observable trends. 


First I will define some functions that will help in the process.



```{r}
tm_cleaner <- function(corpus, stop=stopwords("en"), rm_num=TRUE) {
  # Cleans a corpus object of spaces,numbers,case,stop words, and punctuation
  # Args:
  # Corpus: Object to be cleaned
  # stop=stopwords(""): define dictionary from which corrsponding stopwords will be pulled
  # rm_num: True or False argument
  #
  #Returns: 
  #Cleaned corpus object where number of rows is equal to input cprpus object.
  #
  require(tm)
  corpus <- tm_map(corpus, stripWhitespace)
  corpus <- tm_map(corpus, removeNumbers)
  corpus <- tm_map(corpus, content_transformer(tolower))
  corpus <- tm_map(corpus, removeWords, stop)
  corpus <- tm_map(corpus, removePunctuation)
  corpus <- tm_map(corpus, content_transformer(function(x) gsub("http\\w+", "", x)))
  return(corpus)
}
```

I developed the following function to easily replicate the sentiment analysis process.

```{r}
sentimentoutputbi <- function(dataNYT)  {
#Generates data frame of NYT headlines and binary sentiment analysis results (positive/negative) from the following dictionaries: General Inquirer(Harvard-IV), Loughran-McDonald financial dictionary, and QDAP (polarity words from qdap package).   

#Also adds a variable called "score" where the results of the dictionaries are converted to a numeric factor (1="positive" or -1="negative") and added together to form an aggregate score
  
#Arg:
#dataNYT- dataframe genreated from scrapping with rtimes package 

#Return
#analysis- dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date)    
      
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   
  
headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-convertToBinaryResponse(sentimentnum$SentimentGI)
LM <-convertToBinaryResponse(sentimentnum$SentimentLM)
QDAP <-convertToBinaryResponse(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

GI_num <- ifelse(GI =="positive",1,-1) 
LM_num <- ifelse(LM =="positive",1,-1) 
QDAP_num <- ifelse(QDAP =="positive",1,-1) 

analysis <- data.frame(Score=factor(GI_num + LM_num + QDAP_num),
  GI=factor(GI),
  LM=factor(LM),
  QDAP=factor(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

return(analysis)

}

```


Taking the dataframes generated from query to the Rtimes package and run them through the sentiment analysis function that I just defined.

```{r}
bitdata <- sentimentoutputbi(bit)

cryptdata <- sentimentoutputbi(crypt)

```


Looking at the data, there are a number of headlines that don't seem to have much to do with bitcoin or crypto currency. I will delete rows if no string containing "bitcoin" or "cryptocurrency" exist.

```{r}
View(bitdata)
View(cryptdata)
```


```{r}

library(stringr)
library(dplyr)
cleanbit <- bitdata %>%
  filter(str_detect(Headline, "Bitcoin"))

cleancrypt <- cryptdata %>%
  filter(str_detect(Headline, "Cryptocurrencies"))
```

Graph of Cleanbit (Score of Dataset that concerns headlines that mention bitcoin)

```{r }
library(ggplot2)
ggplot(cleanbit) + 
  geom_point(aes(x=date, y=Score)) + ggtitle("Sentiment Analysis Score", subtitle = "Headlines That Mention Bitcoin") 
```
Graph of Cleancrypt (Score of Dataset that concerns headlines that mention bitcoin)

```{r }
library(ggplot2)
ggplot(cleancrypt) + 
  geom_point(aes(x=date, y=Score)) +  ggtitle("Sentiment Analysis Score", subtitle = "Headlines That Mention cryptocurrency") 
```

There appears to be a high amount of "good" headlines concerning bitcoin (scores of three). This seems to be promising coorelating with the increse in market vaule, but I would like to create some kind of density graph so we can see where media coverage was at its highest.

When I filter headlines in the crypto dataset, based on the presence of the character string "Cryptocurrency"", we find only two points. Therefore, I will focus my work with the Rtimes package on the headlines concerning bitcoin.

Given how well known bitcoin is, I felt safe to proceed this way as "bitcoin" is often discussed as a proxy for cryptocurrencies in general.

```{r }
library(dplyr)
table <- cleanbit %>%
  group_by(date) %>%
  summarise (n = n())
```


Looking at the result, it appears that media intrest from the NYT in Bitcoin, peaked in 2014. This doesnt follow bitcoin prices closely, given the spike in 2017 of not just bitcoin but of other cryptocurrencies as well. 

```{r }
ggplot(table) + geom_density(aes(date,fill="red")) + ggtitle("NYT Interest in Bitcoin", subtitle = "Smoothed")
```


```{r}
freqnews <- function(analyze, word) {
#Function to count number of headlines a day and filter headlines by character string  
#Args:
#analyze -  dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date) [Output from Previously Defined SentimentAnalysis function]
#Word - Character string to filter headlines by  
#Returns:
#freq - Dataframe with Date and N-Number of news articles that share date     
  
    
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


return(freq)

}
```


```{r}
aggregatescore <- function(analyze,word) {
#Function to aggregate the scores of all headlines from a particular date into one aggregate score per date
  
#Args:  
#analyze -  dataframe with Score,GI,LM,QDAP,Headline,Date [Output from Previously Defined SentimentAnalysis function]
#Word - Character string to filter headlines by   
#Returns:  
#sums - dataframe with Date and Corresponding Aggregate Score (sum).  
    
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

return(sums)  
  
}

```


Using the previously defined function to set of bitcoin headlines...


```{r}
cleanbitagg <- aggregatescore(bitdata,"Bitcoin")
```

When we plot with a dataset cleaned for the chracter string bitcoin in the headlines we find that most coverage is positive when we add the score for each date. 

```{r}
library(ggplot2)
ggplot(cleanbitagg) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date') + ggtitle("NYT Bitcoin Coverage", subtitle = "Aggregate Score per Day")
```

In the previous plot I see that there are a number of "good news days" for bitcoin, particularly in 2014.

Let me pull the dates of that have a score larger than 5, and see what they have in common. I assembled a function to do just that.

```{r}

topheadline <- function(analyze,word, condition) {
#Function to pull filters table to include headlines from dates with that have a score within a specified range
#Args:  
#Analyze -Dataframe with Score,GI,LM,QDAP,Headline,Date [Output from Previously Defined SentimentAnalysis function]
#Word - Character string to filter headlines by   
#Condition -  Condition to be met concerning dates that have a aggregate score above a certain integer ex: "sum > 4" 
#Returns:  
#complete - Return  
    
library(stringr)
library(dplyr)
clean <- analyze %>%
  filter(str_detect(Headline, word))

table <- clean %>%
  group_by(date) %>%
  summarise (n = n())


clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums, eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

return(complete)

}

```



```{r}
topheadline<- topheadline(bitdata,"Bitcoin","sum > 5")
```


I notice when I set the condition to a high number, I get identical results, causing some of the outliers in my data. I then decide to alter the sentimentoutput to only generate unique headlines.    
```{r}
View(topheadline)
```


```{r}
sentimentoutputunique <- function(dataNYT)  {
#Generates data frame of NYT headlines and binary sentiment analysis results (positive/negative) from the following dictionaries: General Inquirer(Harvard-IV), Loughran-McDonald financial dictionary, and QDAP (polarity words from qdap package).   

#Also adds a variable called "score" where the results of the dictionaries are converted to a numeric factor (1="positive" or -1="negative") and added together to form an aggregate score
  
#Identical to previous sentimentoutput function except only returns unique headlines  
  
#Arg:
#dataNYT- dataframe genreated from scrapping with rtimes package 

#Return
#analysis- dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date)  
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   

headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-convertToBinaryResponse(sentimentnum$SentimentGI)
LM <-convertToBinaryResponse(sentimentnum$SentimentLM)
QDAP <-convertToBinaryResponse(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

GI_num <- ifelse(GI =="positive",1,-1) 
LM_num <- ifelse(LM =="positive",1,-1) 
QDAP_num <- ifelse(QDAP =="positive",1,-1) 

analysis <- data.frame(Score=factor(GI_num + LM_num + QDAP_num),
  GI=factor(GI),
  LM=factor(LM),
  QDAP=factor(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

analysis2 <- unique(analysis)

return(analysis2)

}
```

Test of the function reduces the total amount of observations from 1153 to 1143. This suggests that the few duplicates that I did see when sorting with the topheadline function with argument "sum > 4", are what contributed to the outliers seen.

```{r}
bitdata <- sentimentoutputunique(bit)
```



Revisiting the previous sentimentoutput funtion, it will be useful to take a look at an apporch that does not reduce the sentiment analysis to a binary. 

The convert to binary function looks at sentiment(dictionary) (i.e. "sentimentQDAP") vaules to make the jugement on if to count a headline as positive or negative. The sentiment vaules themselves are a sum of the positive and negative dictionary vaules (i.e. PositivityGI, NegativityGI) 

So instead of recoding, why not just add the sentiment sum, allowing for nuance that retaining the continuous scale offers.

```{r}
sentimentoutputcont <- function(dataNYT)  {
#Generates data frame of NYT headlines and binary sentiment analysis results (positive/negative) from the following dictionaries: General Inquirer(Harvard-IV), Loughran-McDonald financial dictionary, and QDAP (polarity words from qdap package).   

#Also adds a variable called "score" where the results of the Sentiment dictionary vaules are added together to form an aggregate score.
  
#Returns unique headlines  
  
#Arg:
#dataNYT- dataframe genreated from scrapping with rtimes package 

#Return
#analysis- dataframe with Score(Factor),GI(Factor),LM(Factor),QDAP(Factor),Headline(Factor),Date(Date)   
  
library(rvest)
library(SentimentAnalysis)
library(stringr)
library(dplyr)
library(tm)

data <-  dataNYT$data   

headlines <- data$headline.main

headlinecorpus <- Corpus(VectorSource(headlines))

headlineclean <- tm_cleaner(headlinecorpus)

sentimentnum <- analyzeSentiment(headlineclean)

GI <-(sentimentnum$SentimentGI)
LM <-(sentimentnum$SentimentLM)
QDAP <-(sentimentnum$SentimentQDAP)

headlineoriginal <- factor(data$headline.main)

analysis <- data.frame(Score=as.numeric(GI + LM + QDAP),
  GI=as.numeric(GI),
  LM=as.numeric(LM),
  QDAP=as.numeric(QDAP),
  Headline=factor(data$headline.main),
  date=as.Date(data$pub_date),
  stringsAsFactors=FALSE)

analysis2 <- unique(analysis)

return(analysis2) 

}
```


```{r}
bitdata2 <- sentimentoutputcont(bit) 
```


```{r}
aggnon <- aggregatescore(bitdata2,"Bitcoin")
```

When score is binary (sentimentoutputunique)

```{r}
library(ggplot2)
ggplot(cleanbitagg) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date')
```

When score is nonbinary (sentimentoutputcont)
```{r}
library(ggplot2)
ggplot(aggnon) + geom_point(aes(x=date,y=sum, colour = sum <0)) +
  scale_colour_manual(name = 'Sum < 0', values = setNames(c('red','green'),c(T, F))) +
  xlab('date') + ylab('score of date') + ggtitle("NYT Bitcoin Coverage", subtitle = "Aggregate Score per Day (Continuous)")
```


Using the continuous ratings, I find more nuance in the headline ratings. I decide that I have now refined my headline data, for more precise sentiment analysis.


I decide that functions to create word clouds would be a easy way to look at the frequency of words that appear in the headlines and find what issues appear to drive days with high headline frequency and/or headlines that score on the exterme ends of the sentiment analysis.  

```{r}
wordc <- function(analyze,word,condition,y) {
#Function to generate word cloud from dataframe generated from aformentioned sentimentoutput functions. 
#
#Args:
#Analyze -Dataframe with Score,GI,LM,QDAP,Headline,Date [Output from Previously Defined SentimentAnalysis function]
#  
#Word - Character string to filter headlines by  
#  
#Condition -  Condition to be met concerning dates that have a aggregate score above a certain integer ex: "sum > 4" 
#  
#y - Maximum number of words to be plotted. least frequent terms dropped
# 
#Returns:  
#Wordcloud Plot   
#  
library(tm)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(dplyr)
library(SnowballC)

clean <- analyze %>%
  filter(str_detect(Headline, word))

clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums,eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

Corpus <- Corpus(VectorSource(complete$Headline))


wordcloud(Corpus, max.words = y, random.order = FALSE)

}

```

```{r}
wordc(bitdata2,"Bitcoin","sum > 0.3 ",100)
```


For comparisons sake, I decide to create the word cloud function without a string filter for the headline, wondering what I would get if I did not filter only for headlines containing bitcoin.

```{r}
wordcnofilter <- function(analyze,condition,y) {
#Function to generate word cloud from dataframe generated from aformentioned sentimentoutput functions. 
#
#Args:
#Analyze -Dataframe with Score,GI,LM,QDAP,Headline,Date [Output from Previously Defined SentimentAnalysis function]
#  
#Condition -  Condition to be met concerning dates that have a aggregate score above a #certain integer ex: "sum > 4" 
#  
#y - Maximum number of words to be plotted. least frequent terms dropped
# 
#Returns:  
#Wordcloud Plot   
#    
library(tm)
library(wordcloud)
library(RColorBrewer)
library(stringr)
library(dplyr)
library(SnowballC)

clean <- analyze 

clean$Score <- as.numeric(as.character(clean$Score))

sums <- clean %>%
           group_by(date) %>%
           summarise(sum = sum(Score))

  
dates <- subset(sums,eval(parse(text=condition)), 
select=c(date))  

complete <- merge(dates,clean,by = "date")

Corpus <- Corpus(VectorSource(complete$Headline))


wordcloud(Corpus, max.words = y, random.order = FALSE)

}
```

```{r}
wordcnofilter(bitdata,"sum > 0",100)
```


```{r}
bitsearch <- topheadline(bitdata2,"China", "sum > -5" )

View(bitsearch)
```




















